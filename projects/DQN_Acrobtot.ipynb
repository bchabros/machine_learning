{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attended-novel",
   "metadata": {},
   "source": [
    "# Keras-RL DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "knowing-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a36c49-6545-4617-bcc4-108817dff08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8777d8-187f-48de-a60f-783d3fd7a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-tenant",
   "metadata": {},
   "source": [
    "## Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compound-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Acrobot-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "israeli-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 3\n",
      "Observation Space: (6,)\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape\n",
    "print(f\"Action Space: {env.action_space.n}\")\n",
    "print(f\"Observation Space: {num_observations}\")\n",
    "\n",
    "assert num_actions == 3 and num_observations == (6,) , \"Wrong environment!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-tuesday",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mexican-deputy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                448       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 8,963\n",
      "Trainable params: 8,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(1,)+num_observations))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229ab74-ea87-49ce-8d99-f8d16af236a5",
   "metadata": {},
   "source": [
    "## DQN AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "short-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50_000,\n",
    "                          window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "polished-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr='eps',\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1,\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=150_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "terminal-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model,\n",
    "               nb_steps_warmup=1000,\n",
    "               target_model_update=1000,\n",
    "               batch_size=32,\n",
    "               gamma=0.99,\n",
    "               policy=policy,\n",
    "               memory=memory,\n",
    "               nb_actions=num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "damaged-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=1e-3),\n",
    "            metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-belgium",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adverse-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  999/10000 [=>............................] - ETA: 6s - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Chabi/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:339: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(values, collections.Sequence):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 79s 8ms/step - reward: -1.0000\n",
      "20 episodes - episode_reward: -500.000 [-500.000, -500.000] - loss: 0.029 - mae: 3.440 - mean_q: -5.070 - mean_eps: 0.967\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: -0.9999\n",
      "20 episodes - episode_reward: -497.700 [-500.000, -454.000] - loss: 0.188 - mae: 9.481 - mean_q: -14.020 - mean_eps: 0.910\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: -0.9992\n",
      "22 episodes - episode_reward: -455.500 [-500.000, -239.000] - loss: 0.402 - mae: 14.335 - mean_q: -21.203 - mean_eps: 0.850\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: -0.9986\n",
      "23 episodes - episode_reward: -417.783 [-500.000, -230.000] - loss: 0.678 - mae: 17.856 - mean_q: -26.402 - mean_eps: 0.790\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 87s 9ms/step - reward: -0.9983\n",
      "26 episodes - episode_reward: -391.462 [-500.000, -223.000] - loss: 0.812 - mae: 20.593 - mean_q: -30.447 - mean_eps: 0.730\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 97s 10ms/step - reward: -0.9970\n",
      "32 episodes - episode_reward: -316.469 [-500.000, -178.000] - loss: 0.961 - mae: 22.671 - mean_q: -33.465 - mean_eps: 0.670\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 95s 9ms/step - reward: -0.9964\n",
      "37 episodes - episode_reward: -267.919 [-500.000, -139.000] - loss: 0.919 - mae: 24.025 - mean_q: -35.344 - mean_eps: 0.610\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: -0.9955\n",
      "45 episodes - episode_reward: -221.111 [-403.000, -135.000] - loss: 0.821 - mae: 24.501 - mean_q: -35.937 - mean_eps: 0.550\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: -0.9950\n",
      "50 episodes - episode_reward: -199.640 [-351.000, -103.000] - loss: 0.743 - mae: 25.357 - mean_q: -37.159 - mean_eps: 0.490\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: -0.9939\n",
      "61 episodes - episode_reward: -163.344 [-408.000, -103.000] - loss: 0.639 - mae: 25.405 - mean_q: -37.163 - mean_eps: 0.430\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: -0.9923\n",
      "77 episodes - episode_reward: -128.052 [-201.000, -89.000] - loss: 0.569 - mae: 25.392 - mean_q: -37.058 - mean_eps: 0.370\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: -0.9920\n",
      "80 episodes - episode_reward: -125.050 [-179.000, -81.000] - loss: 0.583 - mae: 24.938 - mean_q: -36.290 - mean_eps: 0.310\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: -0.9912\n",
      "88 episodes - episode_reward: -112.795 [-209.000, -71.000] - loss: 0.563 - mae: 23.816 - mean_q: -34.561 - mean_eps: 0.250\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 118s 12ms/step - reward: -0.9899\n",
      "101 episodes - episode_reward: -97.495 [-300.000, -63.000] - loss: 0.565 - mae: 22.825 - mean_q: -33.048 - mean_eps: 0.190\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: -0.9889\n",
      "done, took 1506.435 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff463bbb4d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, \n",
    "        nb_steps=150_000,\n",
    "        visualize=False,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-nightmare",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9d573bb-b551-4c06-b17d-b475a4cc79ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -62.000, steps: 63\n",
      "Episode 2: reward: -64.000, steps: 65\n",
      "Episode 3: reward: -72.000, steps: 73\n",
      "Episode 4: reward: -81.000, steps: 82\n",
      "Episode 5: reward: -70.000, steps: 71\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env,\n",
    "         nb_episodes=5,\n",
    "         visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bd57a-69b7-42e0-86a6-b5a7e2b3f675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
